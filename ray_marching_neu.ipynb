{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from vgn.networks import load_network\n",
    "import torch\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from vgn.dataset_voxel_grasp_pc import DatasetVoxelGraspPCOcc\n",
    "from pathlib import Path\n",
    "import random\n",
    "import trimesh\n",
    "\n",
    "from vgn.simulation import ClutterRemovalSim\n",
    "from vgn.utils.transform import Rotation, Transform\n",
    "from vgn.utils.implicit import get_scene_from_mesh_pose_list, as_mesh\n",
    "from vgn.perception import *\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "scene = \"d7e7d6e296ec4abfaad79acd252ac9b3\" #\"58c827f23ec34543a72a4e7bc6fe362d\" ##\"58c827f23ec34543a72a4e7bc6fe362d\"\n",
    "root = Path('data/pile/data_pile_train_constructed_4M_HighRes_radomized_views_GPG_only')\n",
    "raw_root = Path('data/pile/data_pile_train_random_raw_4M_radomized_views/')\n",
    "model_path = 'best_models/23-05-01-08-11-39_dataset=data_pile_train_constructed_4M_HighRes_radomized_views,augment=False,net=6d_neu_grasp_pn_deeper,batch_size=32,lr=5e-05,PN_deeper_DIMS_CONT/best_neural_grasp_neu_grasp_pn_deeper_val_acc=0.9097.pt'\n",
    "net = \"neu_grasp_pn_deeper\"\n",
    "net_with_grasp_occ = True\n",
    "\n",
    "camera_eye = np.load('viewpoint_f614e39ed9df4e1094d569cddc20979b.npy')\n",
    "see_table = True\n",
    "data_root = \"/home/hypatia/6D-DAAD/GIGA\"\n",
    "size = 0.3\n",
    "resolution = 64\n",
    "mesh_list_file = os.path.join(raw_root, 'mesh_pose_list', scene + '.npz')\n",
    "mesh_pose_list = np.load(mesh_list_file, allow_pickle=True)['pc']\n",
    "sim = ClutterRemovalSim('pile', 'pile/train', gui=False, data_root=data_root) # parameters scene and object_set are not used\n",
    "sim.setup_sim_scene_from_mesh_pose_list(mesh_pose_list, table=see_table, data_root=data_root) # Setting table to False because we don't want to render it\n",
    "scene_mesh = get_scene_from_mesh_pose_list(mesh_pose_list, data_root=data_root)\n",
    "\n",
    "net = load_network(model_path, device=device, model_type=net)\n",
    "net = net.eval()\n",
    "\n",
    "# NOTE: Renamed \"grasps_with_clouds_gt\".csv to \"grasps_with_clouds.csv\"\n",
    "data = DatasetVoxelGraspPCOcc(root, raw_root, use_grasp_occ=False, num_point_occ=8000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimesh.Scene([scene_mesh]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsq = lambda x: torch.as_tensor(x).unsqueeze(0).float()\n",
    "\n",
    "def load_data(scene):\n",
    "    index = random.choice(data.df[data.df.scene_id==scene].index)\n",
    "    # pc, y, grasp_query, occ_points, occ =  data[index]\n",
    "    pc, (label, width), (pos, rotations, grasps_pc_local, grasps_pc), pos_occ, occ_value = data[index]\n",
    "    pc, label, width, pos, rotations, grasps_pc_local, grasps_pc, pos_occ, occ_value = unsq(pc), unsq(label), unsq(width), unsq(pos), unsq(rotations), unsq(grasps_pc_local), unsq(grasps_pc), unsq(pos_occ), unsq(occ_value)\n",
    "    return pc, (label, width, occ_value), (pos, rotations, grasps_pc_local, grasps_pc), pos_occ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf, y, grasp_query, pos_occ = load_data(scene)\n",
    "\n",
    "# Optional: Load another TSDF from the scene (note the \"see_table\" variable and whether the network was trained with or without the table)\n",
    "def render_n_images(sim, n=1, random=False, noise_type=''):\n",
    "    origin = Transform(Rotation.identity(), np.r_[size / 2, size / 2, 0.0])\n",
    "    if random:\n",
    "        theta = np.random.uniform(0.0, 5* np.pi / 12.0) # elevation: 0 to 75 degrees\n",
    "        # theta = np.random.uniform(5*np.pi/12.0)\n",
    "        # 75 degree reconstruction is unrealistic, try 60\n",
    "        # theta = np.random.uniform(np.pi/3)\n",
    "        # theta = np.random.uniform(np.pi/6, np.pi/4) # elevation: 30 to 45 degrees\n",
    "        r = np.random.uniform(2, 2.4) * size\n",
    "    else:\n",
    "        theta = np.pi / 4.0 # 45 degrees from top view\n",
    "        r = 2.0 * size\n",
    "    \n",
    "    phi_list = 2.0 * np.pi * np.arange(n) / n # circle around the scene\n",
    "    extrinsics = [camera_on_sphere(origin, r, theta, phi) for phi in phi_list]\n",
    "    depth_imgs = []\n",
    "\n",
    "    for extrinsic in extrinsics:\n",
    "        # Multiple views -> for getting other sides of pc\n",
    "        depth_img = sim.camera.render(extrinsic)[1]\n",
    "        # add noise\n",
    "        # depth_img = apply_noise(depth_img, noise_type)\n",
    "        \n",
    "        depth_imgs.append(depth_img)\n",
    "\n",
    "    return depth_imgs, extrinsics\n",
    "\n",
    "# Get random scene image:\n",
    "depth_imgs, extrinsics = render_n_images(sim, n=1, random=True, noise_type='')\n",
    "# Show the image\n",
    "# plt.imshow(depth_imgs[0])\n",
    "\n",
    "# Make tsdf and pc from the image\n",
    "tsdf = TSDFVolume(size, resolution=resolution)\n",
    "for depth_img, extrinsic in zip(depth_imgs, extrinsics):\n",
    "    tsdf.integrate(depth_img, sim.camera.intrinsic, extrinsic)\n",
    "seen_pc = tsdf.get_cloud()\n",
    "# Optional: Crop out table\n",
    "lower = np.array([0.0 , 0.0 , 0.055])\n",
    "upper = np.array([size, size, size])\n",
    "bounding_box = o3d.geometry.AxisAlignedBoundingBox(lower, upper)\n",
    "seen_pc = seen_pc.crop(bounding_box)\n",
    "# convert to torch tensor\n",
    "tsdf = torch.tensor(tsdf.get_grid(), device=device, dtype=torch.float32)\n",
    "\n",
    "# Viz seen point cloud and camera position\n",
    "# seen_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0.64, 0.93]), (np.asarray(seen_pc.points).shape[0], 1)))\n",
    "# cam_pos_pc = o3d.geometry.PointCloud()\n",
    "# cam_pos_pc.points = o3d.utility.Vector3dVector(np.array([extrinsics[0].inverse().translation]))\n",
    "# cam_pos_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0.64, 0.93]), (np.asarray(cam_pos_pc.points).shape[0], 1)))\n",
    "# visualizer.add_geometry(seen_pc)\n",
    "# visualizer.add_geometry(cam_pos_pc)\n",
    "# visualizer.show()\n",
    "\n",
    "\n",
    "out = net(tsdf, grasp_query, p_tsdf=pos_occ)\n",
    "occ = out[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if network gives correct occupancies\n",
    "x, y, z = torch.meshgrid(torch.linspace(start=-0.5, end=0.5 - 1.0 / 64, steps= 64), torch.linspace(start=-0.5, end=0.5 - 1.0 / 64, steps=64), torch.linspace(start=-0.5, end=0.5 - 1.0 / 64, steps=64))\n",
    "        # 1, self.resolution, self.resolution, self.resolution, 3\n",
    "pos = torch.stack((x, y, z), dim=-1).float().unsqueeze(0)\n",
    "pos = pos.view(1, 64*64*64, 3)\n",
    "c = net.encode_inputs(tsdf)\n",
    "occupancies = net.decoder_tsdf(pos, c,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcubes\n",
    "import plotly.graph_objects as go\n",
    "vertices, triangles = mcubes.marching_cubes(occupancies.view(64, 64, 64).detach().numpy(), 0.5)\n",
    "x, y, z = vertices.T\n",
    "i, j, k = triangles.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_eye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Mesh3d(x=x, y=y, z=z,\n",
    "                          i=i, j=j, k=k,\n",
    "                        color='rgb(194, 30, 86)'))\n",
    "camera_eye[2, 3] += 3\n",
    "camera_eye = np.linalg.inv(camera_eye.squeeze())[:3, 3]\n",
    "camera = dict(eye=dict(x=camera_eye[0], y=camera_eye[1], z=camera_eye[2]))\n",
    "fig.update_layout(scene_camera=camera)\n",
    "fig.update_xaxes(tickmode='linear')\n",
    "fig.update_layout(autosize=False,\n",
    "                    width  = 1600,\n",
    "                    height = 1600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices = o3d.utility.Vector3dVector(vertices)\n",
    "# trianlges = o3d.utility.Vector3dVector(triangles)\n",
    "# reconstruction = trimesh.Trimesh(vertices=vertices, faces=triangles)\n",
    "# reconstruction.visual.face_colors = [186, 0, 1, 50]\n",
    "\n",
    "\n",
    "# reconstruction = trimesh.Scene([reconstruction])\n",
    "# reconstruction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = 0.3\n",
    "# camera = reconstruction.camera\n",
    "# cam_resolution = [1920, 1080]\n",
    "# rot_by_x_degrees = 55\n",
    "# # TODO: euler to rotation matrix\n",
    "# pitch_rot = np.array([   [  1.0000000,  0.0000000,  0.0000000, 0.0],\n",
    "#                             [  0.0000000,  0.5735765, -0.8191521, 0.0],\n",
    "#                             [  0.0000000,  0.8191521,  0.5735765, 0.0],\n",
    "#                             [  0.0000000,  0.0000000,  0.0000000, 1.0]   ])\n",
    "# distance = 0.75\n",
    "# camera_tf = camera.look_at(points=[[size/2,size/2,0.06]], rotation=pitch_rot, distance=distance)\n",
    "# reconstruction.camera_transform = camera_tf\n",
    "# reconstruction.camera.resolution = cam_resolution\n",
    "# reconstruction.show(line_settings= {'point_size': 8})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0691386a9d48df7c05a3616a951f00a5f34f293c8d5f5aed00d87c655274d97a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
