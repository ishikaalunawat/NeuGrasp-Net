{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f381c1b0ad0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from vgn.networks import load_network\n",
    "import torch\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from vgn.dataset_voxel_grasp_pc import DatasetVoxelGraspPCOcc\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading [neu_grasp_pn_deeper] model from data/best_runs/23-05-03-01-42-37_dataset=data_pile_train_constructed_4M_HighRes_radomized_views_no_table,augment=False,net=6d_neu_grasp_pn_deeper,batch_size=32,lr=5e-05,PN_no_tab_deeper_DIMS_WITH_occ/best_neural_grasp_neu_grasp_pn_deeper_val_acc=0.9120.pt\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "scene = \"0a0b691837b745568fa6f98d546027ed\"\n",
    "root = Path('data/pile/data_pile_train_constructed_4M_HighRes_radomized_views_GPG_only')\n",
    "raw_root = Path('data/pile/data_pile_train_random_raw_4M_radomized_views/')\n",
    "model_path = 'data/best_runs/23-05-03-01-42-37_dataset=data_pile_train_constructed_4M_HighRes_radomized_views_no_table,augment=False,net=6d_neu_grasp_pn_deeper,batch_size=32,lr=5e-05,PN_no_tab_deeper_DIMS_WITH_occ/best_neural_grasp_neu_grasp_pn_deeper_val_acc=0.9120.pt'\n",
    "net = \"neu_grasp_pn_deeper\"\n",
    "net_with_grasp_occ = True\n",
    "\n",
    "net = load_network(model_path, device=device, model_type=net)\n",
    "\n",
    "# NOTE: Renamed \"grasps_with_clouds_gt\".csv to \"grasps_with_clouds.csv\"\n",
    "data = DatasetVoxelGraspPCOcc(root, raw_root, use_grasp_occ=net_with_grasp_occ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_plotly(geometries, camera_eye=[1.25, 1.25, 1.25]):\n",
    "    '''\n",
    "    geometries (numpy arrays): [Voxels, PCD1, PCD2, ... PCDN]\n",
    "    If mesh:\n",
    "    geometries [dictionary ; numpy arrays] : [Dict, Surface points] \n",
    "\n",
    "    Note: If you do not want to plot Voxels, pass `None`\n",
    "    '''\n",
    "\n",
    "    fig = make_subplots(\n",
    "                        rows=1, cols=2,\n",
    "                        specs=[[{'type': 'scene'}, {'type': 'scene'}]])\n",
    "    # IF: TSDF VOXELS\n",
    "    Voxels = VoxelData(geometries[0])\n",
    "    voxels = go.Mesh3d(\n",
    "            x=Voxels.vertices[0],\n",
    "            y=Voxels.vertices[1],\n",
    "            z=Voxels.vertices[2],\n",
    "            i=Voxels.triangles[0],\n",
    "            j=Voxels.triangles[1],\n",
    "            k=Voxels.triangles[2]\n",
    "            )\n",
    "\n",
    "    fig.add_trace(voxels, row=1, col=1)\n",
    "    # Surface points\n",
    "    for i in range(1, len(geometries)):\n",
    "        pcd = go.Scatter3d(\n",
    "                        x=geometries[i][:,0], \n",
    "                        y=geometries[i][:,1], \n",
    "                        z=geometries[i][:,2], \n",
    "                        marker=go.scatter3d.Marker(size=4), \n",
    "                        opacity=0.8, \n",
    "                        mode='markers')\n",
    "        # data.append(pcd)\n",
    "        fig.add_trace(pcd, row=1, col=2)\n",
    "\n",
    "    print(\"Generating figure\")\n",
    "    # fig = go.Figure(data=data)\n",
    "    fig.update_xaxes(tickmode='linear')\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(scene):\n",
    "    index = random.choice(data.df[data.df.scene_id==scene].index)\n",
    "    pc, y, grasp_query, occ_points, occ =  data[index]\n",
    "    return pc, y, grasp_query, occ_points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/pile/data_pile_train_random_raw_4M_radomized_views/occ_grasp_point_clouds_noisy/1076741.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blab/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m x, y, grasp_query, pos_occ \u001b[39m=\u001b[39m load_data(scene)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blab/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m net(x, grasp_query, p_tsdf\u001b[39m=\u001b[39mpos_occ)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blab/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m sdf \u001b[39m=\u001b[39m out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb Cell 8\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(scene)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blab/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m(scene):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blab/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(data\u001b[39m.\u001b[39mdf[data\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mscene_id\u001b[39m==\u001b[39mscene]\u001b[39m.\u001b[39mindex)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blab/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     pc, y, grasp_query, occ_points, occ \u001b[39m=\u001b[39m  data[index]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blab/home/hypatia/6D-DAAD/GIGA/ray_marching_neu.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pc, y, grasp_query, occ_points\n",
      "File \u001b[0;32m~/6D-DAAD/GIGA/src/vgn/dataset_voxel_grasp_pc.py:117\u001b[0m, in \u001b[0;36mDatasetVoxelGraspPCOcc.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    114\u001b[0m occ_points \u001b[39m=\u001b[39m occ_points \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_grasp_occ \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Also load the occupancy data of grasp clouds\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     grasp_pc_occ_points, grasp_pc_occ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_grasp_pc_occ(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49mloc[i, \u001b[39m\"\u001b[39;49m\u001b[39mgrasp_id\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    118\u001b[0m     grasp_pc_occ_points \u001b[39m=\u001b[39m grasp_pc_occ_points \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m    119\u001b[0m     \u001b[39m# Make sure size is always the same. Pad zeros\u001b[39;00m\n",
      "File \u001b[0;32m~/6D-DAAD/GIGA/src/vgn/dataset_voxel_grasp_pc.py:138\u001b[0m, in \u001b[0;36mDatasetVoxelGraspPCOcc.read_grasp_pc_occ\u001b[0;34m(self, grasp_id)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_grasp_pc_occ\u001b[39m(\u001b[39mself\u001b[39m, grasp_id):\n\u001b[1;32m    137\u001b[0m     grasp_pc_occ_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_root \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mocc_grasp_point_clouds_noisy\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m (\u001b[39mstr\u001b[39m(grasp_id) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m     grasp_pc_occ_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(grasp_pc_occ_path)\n\u001b[1;32m    139\u001b[0m     points \u001b[39m=\u001b[39m grasp_pc_occ_data[\u001b[39m'\u001b[39m\u001b[39mpoints\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    140\u001b[0m     occ \u001b[39m=\u001b[39m grasp_pc_occ_data[\u001b[39m'\u001b[39m\u001b[39mocc\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/giga/lib/python3.8/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/pile/data_pile_train_random_raw_4M_radomized_views/occ_grasp_point_clouds_noisy/1076741.npz'"
     ]
    }
   ],
   "source": [
    "x, y, grasp_query, pos_occ = load_data(scene)\n",
    "out = net(x, grasp_query, p_tsdf=pos_occ)\n",
    "sdf = out[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, grasp_query, pos_occ = prepare_batch(batch, device) # <- Changed to predict only grasp quality (check inside)\n",
    "        y_pred = select(net(x, grasp_query, p_tsdf=pos_occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Check if network gives correct occupancies\n",
    "# x, y, z = torch.meshgrid(torch.linspace(start=-0.5, end=0.5 - 1.0 / 40, steps= 40), torch.linspace(start=-0.5, end=0.5 - 1.0 / 40, steps=40), torch.linspace(start=-0.5, end=0.5 - 1.0 / 40, steps=40))\n",
    "#         # 1, self.resolution, self.resolution, self.resolution, 3\n",
    "# pos = torch.stack((x, y, z), dim=-1).float().unsqueeze(0)\n",
    "# pos = pos.view(1, 40 * 40 * 40, 3)\n",
    "# occupancies_64K = net.renderer.model(pos, torch.Tensor(tsdf), only_occupancy=True)\n",
    "# import mcubes\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# vertices, triangles = mcubes.marching_cubes(occupancies_64K.view(40, 40, 40).detach().numpy(), 0.5)\n",
    "# x, y, z = vertices.T\n",
    "# i, j, k = triangles.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(go.Mesh3d(x=x, y=y, z=z,\n",
    "#                           i=i, j=j, k=k,\n",
    "#                           intensity=z))\n",
    "# fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize TSDF with only GT pointcloud, occupied points \n",
    "visualize_plotly([tsdf.squeeze(), pc, points_occ])\n",
    "\n",
    "## Visualize TSDF with GT pointcloud, occupied points, image plane and camera-pos\n",
    "# points_occ_shifted = points_occ.clone().detach()\n",
    "# points_occ_shifted[:, 0] = points_occ_shifted[:, 0] - 0.03 # Shifting x-axis\n",
    "# points_occ_shifted[:, 1] = points_occ_shifted[:, 1] - 0.025 # Shifting y-axis\n",
    "# visualize_plotly([tsdf, pc, points_occ_shifted, pixels_world[0], cam_position.reshape(1, 3)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val.view(batch_size, -1, n_steps)\n",
    "mask_0_not_occupied = val[:, :, 0] < 0\n",
    "sign_matrix = torch.cat([torch.sign(val[:, :, :-1] * val[:, :, 1:]),\n",
    "                                 torch.ones(batch_size, n_pts, 1)],\n",
    "                                dim=-1)\n",
    "cost_matrix = sign_matrix * torch.arange(n_steps, 0, -1).float()\n",
    "values, indices = torch.min(cost_matrix, -1)\n",
    "mask_sign_change = values < 0\n",
    "mask_neg_to_pos = val[torch.arange(batch_size).unsqueeze(-1),torch.arange(n_pts).unsqueeze(-0), indices] < 0\n",
    "mask = mask_sign_change & mask_neg_to_pos & mask_0_not_occupied\n",
    "print(\"Mask truth:\", sum(sum(mask==True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant(tsdf, f_low, f_high, d_low, d_high, n_secant_steps,\n",
    "                          ray0_masked, ray_direction_masked, tau, it=0):\n",
    "    ''' Runs the secant method for interval [d_low, d_high].\n",
    "\n",
    "    Args:\n",
    "        d_low (tensor): start values for the interval\n",
    "        d_high (tensor): end values for the interval\n",
    "        n_secant_steps (int): number of steps\n",
    "        ray0_masked (tensor): masked ray start points\n",
    "        ray_direction_masked (tensor): masked ray direction vectors\n",
    "        model (nn.Module): model model to evaluate point occupancies\n",
    "        c (tensor): latent conditioned code c\n",
    "        tau (float): threshold value in logits\n",
    "    '''\n",
    "    d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\n",
    "    for i in range(n_secant_steps):\n",
    "        p_mid = ray0_masked + d_pred.unsqueeze(-1) * ray_direction_masked\n",
    "        p_scaled_mid = p_mid/0.3 - 0.5\n",
    "\n",
    "        with torch.no_grad():\n",
    "            f_mid = (net.renderer.model(p_scaled_mid.view(1, -1, 3),  tsdf, \n",
    "                            only_occupancy=True)- tau).squeeze()\n",
    "        ind_low = f_mid < 0\n",
    "        ind_low = ind_low\n",
    "        if ind_low.sum() > 0:\n",
    "            d_low[ind_low] = d_pred[ind_low]\n",
    "            f_low[ind_low] = f_mid[ind_low]\n",
    "        if (ind_low == 0).sum() > 0:\n",
    "            d_high[ind_low == 0] = d_pred[ind_low == 0]\n",
    "            f_high[ind_low == 0] = f_mid[ind_low == 0]\n",
    "\n",
    "        d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\n",
    "    return d_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = batch_size * n_pts\n",
    "d_low = d_proposal.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "f_low = val.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(\n",
    "            batch_size, n_pts)[mask]\n",
    "indices = torch.clamp(indices + 1, max=n_steps-1)\n",
    "d_high = d_proposal.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "f_high = val.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray0_masked = camera_world[mask]\n",
    "ray_direction_masked = ray_vector[mask]\n",
    "\n",
    "# Apply surface depth refinement step (e.g. Secant method)\n",
    "d_pred = secant(torch.Tensor(tsdf),\n",
    "    f_low, f_high, d_low, d_high, n_secant_steps, ray0_masked,\n",
    "    ray_direction_masked, tau = 0.5)\n",
    "\n",
    "# for sanity\n",
    "d_pred_out = torch.ones(batch_size, n_pts)\n",
    "d_pred_out[mask] = d_pred\n",
    "d_pred_out[mask == 0] = np.inf\n",
    "d_pred_out[mask_0_not_occupied == 0] = 0\n",
    "d_i = d_pred_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(tensor):\n",
    "    ''' Returns mask of non-illegal values for tensor.\n",
    "\n",
    "    Args:\n",
    "        tensor (tensor): Numpy or Pytorch tensor\n",
    "    '''\n",
    "    # tensor, is_numpy = to_pytorch(tensor, True)\n",
    "    mask = ((abs(tensor) != np.inf) & (torch.isnan(tensor) == False))\n",
    "    mask = mask.bool()\n",
    "    # if is_numpy:\n",
    "    #     mask = mask.numpy()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_proposal[mask, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_zero_occupied = d_i == 0\n",
    "d_i = d_i.detach()\n",
    "\n",
    "# Get mask for predicted depth\n",
    "mask_pred = get_mask(d_i).detach()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dists = torch.ones_like(d_i)\n",
    "    dists[mask_pred] = d_i[mask_pred]\n",
    "    dists[mask_zero_occupied] = 0.\n",
    "    network_object_mask = mask_pred & ~mask_zero_occupied\n",
    "    network_object_mask = network_object_mask[0]\n",
    "    dists = dists[0]\n",
    "\n",
    "# Project depth to 3d poinsts\n",
    "camera_world = camera_world.reshape(-1, 3)\n",
    "ray_vector = ray_vector.reshape(-1, 3)\n",
    "\n",
    "points = camera_world + ray_vector * dists.unsqueeze(-1)\n",
    "points = points.view(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_mask = network_object_mask.view(-1)\n",
    "surface_points = points[surface_mask]\n",
    "N = surface_points.shape[0]\n",
    "surface_points_neig = surface_points + (torch.rand_like(surface_points) - 0.5) * 0.01\n",
    "pp = torch.cat([surface_points, surface_points_neig], dim=0)\n",
    "pp_scaled = (pp - torch.min(pp))/(torch.max(pp) - torch.min(pp)) - 0.5\n",
    "g = net.renderer.model.gradient(torch.Tensor(tsdf), pp_scaled.view(1, -1, 3)).permute(1, 0, 2) #Unisurf in [n_pts, 1, 3]\n",
    "# print(g.size())\n",
    "normals_ = g[:, 0, :] / (g[:, 0, :].norm(2, dim=1).unsqueeze(-1) + 10**(-5)) \n",
    "diff_norm =  torch.norm(normals_[:N] - normals_[N:], dim=-1)\n",
    "# surface_points = p_proposal[mask, :, :].view(-1, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_plotly([tsdf.squeeze(), pc, surface_points.reshape(-1, 3)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0691386a9d48df7c05a3616a951f00a5f34f293c8d5f5aed00d87c655274d97a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
