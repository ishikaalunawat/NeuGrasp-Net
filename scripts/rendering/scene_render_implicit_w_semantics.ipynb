{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading [neu_grasp_pn_affnet_sem] model from /home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/runs_relevant_affnet/23-11-08-03-40-33_dataset=data_affnet_train_constructed_GPG_60,augment=False,net=6d_neu_grasp_pn_affnet_sem,batch_size=64,lr=2e-04,AFFNET_SEM_v4_no_hand_balanced_CONT/best_neural_grasp_neu_grasp_pn_affnet_sem_val_acc=0.9039.pt\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import open3d as o3d\n",
    "from open3d import JVisualizer\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vgn.grasp import Grasp, Label\n",
    "from vgn.io import *\n",
    "from vgn.perception import *\n",
    "from vgn.simulation import ClutterRemovalSim\n",
    "from vgn.utils.transform import Rotation, Transform\n",
    "from vgn.utils.implicit import get_scene_from_mesh_pose_list, as_mesh\n",
    "from vgn.utils.misc import apply_noise\n",
    "from vgn.grasp_sampler import GpgGraspSamplerPcl\n",
    "from vgn.networks import get_network, load_network\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "constructed_root = Path(\"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/3DGraspAff/data_affnet_train_constructed_GPG_60\")\n",
    "model_type = \"neu_grasp_pn_affnet_sem\"\n",
    "model_path = \"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/runs_relevant_affnet/23-11-08-03-40-33_dataset=data_affnet_train_constructed_GPG_60,augment=False,net=6d_neu_grasp_pn_affnet_sem,batch_size=64,lr=2e-04,AFFNET_SEM_v4_no_hand_balanced_CONT/best_neural_grasp_neu_grasp_pn_affnet_sem_val_acc=0.9039.pt\"\n",
    "device = \"cuda\"\n",
    "net = load_network(model_path, device, model_type)\n",
    "\n",
    "previous_root=\"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/3DGraspAff/data_affnet_train_random_raw_GPG_60\"\n",
    "data_root = \"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/\"\n",
    "\n",
    "sim_gui = False\n",
    "three_cameras = True # Use one camera for wrist and two cameras for the fingers\n",
    "add_noise = False # Add dex noise to the rendered images like GIGA\n",
    "noise_type = 'mod_dex'\n",
    "size = 0.3 # 30cm\n",
    "resolution = 64\n",
    "scene_voxel_downsample_size = 0.005 # 5mm\n",
    "max_points = 1023\n",
    "scene='affnet'#'pile'\n",
    "object_set='test'#'pile/train'\n",
    "\n",
    "## Re-create the saved simulation\n",
    "# Get random scene\n",
    "index = np.random.randint(32000) # index 20 is a good example.\n",
    "mesh_list_files = glob.glob(os.path.join(previous_root, 'mesh_pose_list', '*.npz'))\n",
    "mesh_pose_list = np.load(mesh_list_files[index], allow_pickle=True)['pc']\n",
    "scene_id = os.path.basename(mesh_list_files[index])[:-4] # scene id without .npz extension\n",
    "## Get specific scene\n",
    "# scene_id = 'f614e39ed9df4e1094d569cddc20979b'\n",
    "# mesh_list_file = os.path.join(previous_root, 'mesh_pose_list', scene_id + '.npz')\n",
    "# mesh_pose_list = np.load(mesh_list_file, allow_pickle=True)['pc']\n",
    "\n",
    "sim = ClutterRemovalSim(scene, object_set, gui=sim_gui, data_root=data_root) # parameters scene and object_set are not used\n",
    "sim.setup_sim_scene_from_mesh_pose_list(mesh_pose_list, table=True, data_root=data_root) # Set table to False iF we don't want to render it\n",
    "# sim.save_state()\n",
    "\n",
    "# Get scene point cloud and normals using ground truth meshes\n",
    "scene_mesh = get_scene_from_mesh_pose_list(mesh_pose_list, data_root=data_root)\n",
    "o3d_scene_mesh = scene_mesh.as_open3d\n",
    "o3d_scene_mesh.compute_vertex_normals()\n",
    "pc = o3d_scene_mesh.sample_points_uniformly(number_of_points=3000)\n",
    "points = np.asarray(pc.points)\n",
    "# pc_trimesh = trimesh.points.PointCloud(points)\n",
    "# pc_colors = np.array([trimesh.visual.random_color() for i in points])\n",
    "# pc_trimesh.vertices_color = pc_colors\n",
    "# trimesh.Scene([scene_mesh, pc_trimesh]).show()\n",
    "# o3d.visualization.draw_geometries([pc])\n",
    "visualizer = JVisualizer()\n",
    "pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0, 0]), (np.asarray(pc.points).shape[0], 1)))\n",
    "visualizer.add_geometry(pc)\n",
    "# visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b8394a2f4a40df9270af879788fae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 3 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_n_images(sim, n=1, random=False, noise_type=''):\n",
    "    origin = Transform(Rotation.identity(), np.r_[size / 2, size / 2, 0.0])\n",
    "    if random:\n",
    "        # theta = np.random.uniform(0.0, 5* np.pi / 12.0) # elevation: 0 to 75 degrees\n",
    "        theta = np.random.uniform(5*np.pi/12.0)\n",
    "        # 75 degree reconstruction is unrealistic, try 60\n",
    "        # theta = np.random.uniform(np.pi/3)\n",
    "        # theta = np.random.uniform(np.pi/6, np.pi/4) # elevation: 30 to 45 degrees\n",
    "        r = np.random.uniform(2, 2.4) * size\n",
    "    else:\n",
    "        theta = np.pi / 60.0 #  30 degrees from top view\n",
    "        r = 2.0 * size\n",
    "    \n",
    "    phi_list = 2.0 * np.pi * np.arange(n) / n # circle around the scene\n",
    "    extrinsics = [camera_on_sphere(origin, r, theta, phi) for phi in phi_list]\n",
    "    depth_imgs = []\n",
    "\n",
    "    for extrinsic in extrinsics:\n",
    "        # Multiple views -> for getting other sides of pc\n",
    "        depth_img = sim.camera.render(extrinsic)[1]\n",
    "        # add noise\n",
    "        depth_img = apply_noise(depth_img, noise_type)\n",
    "        \n",
    "        depth_imgs.append(depth_img)\n",
    "\n",
    "    return depth_imgs, extrinsics\n",
    "\n",
    "# Get random scene image:\n",
    "depth_imgs, extrinsics = render_n_images(sim, n=1, random=False, noise_type='')\n",
    "# Show the image\n",
    "# plt.imshow(depth_imgs[0])\n",
    "\n",
    "# Make tsdf and pc from the image\n",
    "tsdf = TSDFVolume(size, resolution)\n",
    "for depth_img, extrinsic in zip(depth_imgs, extrinsics):\n",
    "    tsdf.integrate(depth_img, sim.camera.intrinsic, extrinsic)\n",
    "seen_pc = tsdf.get_cloud()\n",
    "# Optional: Crop out table\n",
    "lower = np.array([0.0 , 0.0 , 0.055])\n",
    "upper = np.array([size, size, size])\n",
    "bounding_box = o3d.geometry.AxisAlignedBoundingBox(lower, upper)\n",
    "seen_pc = seen_pc.crop(bounding_box)\n",
    "\n",
    "# Viz seen point cloud and camera position\n",
    "seen_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0.64, 0.93]), (np.asarray(seen_pc.points).shape[0], 1)))\n",
    "cam_pos_pc = o3d.geometry.PointCloud()\n",
    "cam_pos_pc.points = o3d.utility.Vector3dVector(np.array([extrinsics[0].inverse().translation]))\n",
    "cam_pos_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0.64, 0.93]), (np.asarray(cam_pos_pc.points).shape[0], 1)))\n",
    "visualizer.add_geometry(seen_pc)\n",
    "visualizer.add_geometry(cam_pos_pc)\n",
    "visualizer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a59f0bb4bd542548c5686dd7f459b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get views from a circular path around the scene\n",
    "depth_imgs_full, extrinsics_full = render_n_images(sim, n=6, random=False, noise_type='')\n",
    "\n",
    "# Make tsdf and pc from the images\n",
    "tsdf_full = TSDFVolume(size, resolution)\n",
    "for depth_img, extrinsic in zip(depth_imgs_full, extrinsics_full):\n",
    "    tsdf_full.integrate(depth_img, sim.camera.intrinsic, extrinsic)\n",
    "pc_full = tsdf_full.get_cloud()\n",
    "\n",
    "# Viz seen point cloud and camera position\n",
    "pc_full.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0.93, 0.63]), (np.asarray(pc_full.points).shape[0], 1)))\n",
    "visualizer2 = JVisualizer()\n",
    "visualizer2.add_geometry(seen_pc)\n",
    "visualizer2.add_geometry(pc_full)\n",
    "visualizer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_proposal(ray0, ray_direction, n_steps=256, depth_range=[0.001, 0.18]):\n",
    "    \n",
    "    d_proposal = torch.linspace(0, 1, steps=n_steps).view(1, 1, n_steps, 1)\n",
    "    d_proposal = depth_range[0] * (1. - d_proposal) + depth_range[1]* d_proposal\n",
    "\n",
    "    p_proposal = ray0.unsqueeze(2).repeat(1, 1, n_steps, 1) + ray_direction.unsqueeze(2).repeat(1, 1, n_steps, 1) * d_proposal\n",
    "\n",
    "    return p_proposal, d_proposal\n",
    "\n",
    "def secant(net, tsdf, f_low, f_high, d_low, d_high, n_secant_steps,\n",
    "                          ray0_masked, ray_direction_masked, tau):\n",
    "    ''' Runs the secant method for interval [d_low, d_high].\n",
    "\n",
    "    Args:\n",
    "        d_low (tensor): start values for the interval\n",
    "        d_high (tensor): end values for the interval\n",
    "        n_secant_steps (int): number of steps\n",
    "        ray0_masked (tensor): masked ray start points\n",
    "        ray_direction_masked (tensor): masked ray direction vectors\n",
    "        model (nn.Module): model model to evaluate point occupancies\n",
    "        c (tensor): latent conditioned code c\n",
    "        tau (float): threshold value in logits\n",
    "    '''\n",
    "    d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\n",
    "    for i in range(n_secant_steps):\n",
    "        p_mid = ray0_masked + d_pred.unsqueeze(-1) * ray_direction_masked\n",
    "        p_scaled_mid = (p_mid/0.3 - 0.5).to(tsdf.device).float()\n",
    "        with torch.no_grad():\n",
    "            f_mid = (net.infer_occ(p_scaled_mid.view(1, -1, 3),  tsdf)- tau).squeeze()#.cpu()\n",
    "            sem_surf_vals = net.infer_sem(p_scaled_mid.view(1, -1, 3),  tsdf).squeeze()\n",
    "        ind_low = f_mid < 0\n",
    "        ind_low = ind_low\n",
    "        if ind_low.sum() > 0:\n",
    "            d_low[ind_low] = d_pred[ind_low]\n",
    "            f_low[ind_low] = f_mid[ind_low]\n",
    "        if (ind_low == 0).sum() > 0:\n",
    "            d_high[ind_low == 0] = d_pred[ind_low == 0]\n",
    "            f_high[ind_low == 0] = f_mid[ind_low == 0]\n",
    "\n",
    "        d_pred = - f_low * (d_high - d_low) / (f_high - f_low) + d_low\n",
    "    return d_pred, sem_surf_vals\n",
    "\n",
    "def render_occ(intrinsic, extrinsics, net, input_tsdf, min_measured_depth, max_measured_depth, n_steps=256):\n",
    "\n",
    "    width, height = (intrinsic.K[:2, 2]*2).astype(int)\n",
    "    batch_size = n_images = 1 # Use 1 for now because GPU memory len(extrinsics)\n",
    "    n_pts = width*height\n",
    "\n",
    "    ## Generate proposal points\n",
    "\n",
    "    # Make pixel points\n",
    "    pixel_grid = torch.meshgrid(torch.arange(width), torch.arange(height))\n",
    "    pixels = torch.dstack((pixel_grid[0],pixel_grid[1])).reshape(-1, 2)\n",
    "    pixels_hom = torch.hstack((pixels, torch.ones((pixels.shape[0], 2)))) # Homogenous co-ordinates\n",
    "\n",
    "    cam_inv_extrinsics = torch.ones((batch_size, 4, 4), dtype=torch.float32)\n",
    "    camera_world = torch.zeros((batch_size, n_pts, 3))\n",
    "    # Using only first camera for now (GPU memory)\n",
    "    # for idx, cam_extrinsic in enumerate(extrinsics):\n",
    "    camera_world[0, :] = torch.from_numpy(extrinsics[0].inverse().translation).float()\n",
    "    cam_inv_extrinsics[0] = torch.from_numpy(extrinsics[0].inverse().as_matrix()).float()\n",
    "\n",
    "    # Convert pixel points to world co-ords:\n",
    "    intrinsic_hom = torch.eye(4)\n",
    "    intrinsic_hom[:3,:3] = torch.tensor(intrinsic.K)\n",
    "    image_plane_depth = 0.01 # 10mm\n",
    "    pixels_hom[:,:3] *= image_plane_depth # Multiply by depth\n",
    "    pixels_local_hom = (torch.inverse(intrinsic_hom) @ pixels_hom.T).unsqueeze(0).repeat(batch_size,1,1)\n",
    "    pixels_world = torch.bmm(cam_inv_extrinsics, pixels_local_hom)[:,:3,:].transpose(1,2)\n",
    "\n",
    "    ray_vector_world = (pixels_world - camera_world)\n",
    "    ray_vector_world = ray_vector_world/ray_vector_world.norm(2,2).unsqueeze(-1)\n",
    "\n",
    "    p_proposal_world, d_proposal = get_simple_proposal(camera_world,\n",
    "                                                    ray_vector_world,\n",
    "                                                    n_steps=n_steps,\n",
    "                                                    depth_range=[min_measured_depth, max_measured_depth])\n",
    "    # Debug: Viz proposal points\n",
    "    # prop_o3d = o3d.geometry.PointCloud()\n",
    "    # max_points = 2000\n",
    "    # points = p_proposal_world[0].view(-1,3)\n",
    "    # indices = np.random.randint(points.shape[0], size=max_points)\n",
    "    # prop_o3d.points = o3d.utility.Vector3dVector(points[indices].numpy())\n",
    "    # prop_o3d.colors = o3d.utility.Vector3dVector(np.random.uniform(0,1,size=(max_points,3)))\n",
    "    # visualizer3 = JVisualizer()\n",
    "    # visualizer3.add_geometry(prop_o3d)\n",
    "    # visualizer3.add_geometry(pc_full)\n",
    "    # visualizer3.show()\n",
    "\n",
    "    # Normalize and convert to query for network\n",
    "    p_scaled_proposal_query = ((p_proposal_world)/size - 0.5).view(batch_size,-1, 3)\n",
    "    # Query network\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        val = net.infer_occ(p_scaled_proposal_query.clone().to(device), input_tsdf)\n",
    "    val = val.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    val = (val - 0.5) # Center occupancies around 0\n",
    "\n",
    "    # (Optional) get semantics of surface points too!\n",
    "    with torch.no_grad():\n",
    "        sem_val = net.infer_sem(p_scaled_proposal_query.clone().to(device), input_tsdf)\n",
    "    sem_val = sem_val.cpu()\n",
    "\n",
    "    # Debug: Viz occupancied points\n",
    "    # points_occ = p_proposal_world.view(-1, 3)[val.squeeze()>0]\n",
    "    # occ_pc = o3d.geometry.PointCloud()\n",
    "    # occ_pc.points = o3d.utility.Vector3dVector(points_occ)\n",
    "    # occ_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.6, 0.0, 1]), (np.asarray(occ_pc.points).shape[0], 1)))\n",
    "    # visualizer4 = JVisualizer()\n",
    "    # visualizer4.add_geometry(occ_pc)\n",
    "    # visualizer4.add_geometry(pc_full)\n",
    "    # visualizer4.show()\n",
    "\n",
    "    ## Surface rendering\n",
    "    val = val.reshape(batch_size, -1, n_steps)\n",
    "    mask_0_not_occupied = val[:, :, 0] < 0\n",
    "    sign_matrix = torch.cat([torch.sign(val[:, :, :-1] * val[:, :, 1:]),\n",
    "                                    torch.ones(batch_size, n_pts, 1)],\n",
    "                                    dim=-1)\n",
    "    cost_matrix = sign_matrix * torch.arange(n_steps, 0, -1).float()\n",
    "    values, indices = torch.min(cost_matrix, -1)\n",
    "    mask_sign_change = values < 0\n",
    "    mask_neg_to_pos = val[torch.arange(batch_size).unsqueeze(-1),torch.arange(n_pts).unsqueeze(-0), indices] < 0\n",
    "    mask = mask_sign_change & mask_neg_to_pos & mask_0_not_occupied\n",
    "\n",
    "    n = batch_size * n_pts\n",
    "    d_proposal = d_proposal.repeat(1,n_pts,1,1)\n",
    "    d_low = d_proposal.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "    f_low = val.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(\n",
    "                batch_size, n_pts)[mask]\n",
    "    indices = torch.clamp(indices + 1, max=n_steps-1)\n",
    "    d_high = d_proposal.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "    f_high = val.view(n, n_steps, 1)[torch.arange(n), indices.view(n)].view(batch_size, n_pts)[mask]\n",
    "\n",
    "    ray0_masked = camera_world[mask]\n",
    "    ray_direction_masked = ray_vector_world[mask]\n",
    "\n",
    "    # Apply surface depth refinement step (e.g. Secant method)\n",
    "    n_secant_steps = 8\n",
    "    with torch.no_grad():\n",
    "        d_pred, sem_surf_vals = secant(net, input_tsdf,\n",
    "            f_low.cuda(), f_high.cuda(), d_low.cuda(), d_high.cuda(), n_secant_steps, ray0_masked.cuda(),\n",
    "            ray_direction_masked.cuda(), tau=torch.tensor(0.5, device=device))\n",
    "    d_pred = d_pred.cpu()\n",
    "    sem_surf_vals = sem_surf_vals.cpu()\n",
    "    points_out = torch.zeros(batch_size, n_pts, 3, dtype=torch.float)#*t_nan # set default (invalid) values\n",
    "    points_out[mask] = ray0_masked + ray_direction_masked * d_pred.unsqueeze(1)\n",
    "    sem_points_out = torch.zeros(batch_size, n_pts, 7, dtype=torch.float)\n",
    "    sem_points_out[mask] = sem_surf_vals\n",
    "\n",
    "    return points_out, mask, sem_points_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render the scene using the occupancy network with the same extrinsics\n",
    "\n",
    "# Neural render camera settings\n",
    "width = 64\n",
    "height = 64\n",
    "width_fov  = np.deg2rad(60) # angular FOV (120 by default)\n",
    "height_fov = np.deg2rad(60) # angular FOV (120 by default)\n",
    "f_x = width  / (np.tan(width_fov / 2.0))\n",
    "f_y = height / (np.tan(height_fov / 2.0))\n",
    "intrinsic = CameraIntrinsic(width, height, f_x, f_y, width/2, height/2)\n",
    "\n",
    "min_measured_depth = size\n",
    "max_measured_depth = 2.4*size + size/2 # max distance from the origin\n",
    "\n",
    "tsdf_t = torch.tensor(tsdf.get_grid(), device=device, dtype=torch.float32)\n",
    "\n",
    "surface_points_combined = None\n",
    "sem_surface_points_combined = None\n",
    "for cam_extrinsic in extrinsics_full:\n",
    "    surf_points_world, surf_mask, sem_points_out = render_occ(intrinsic, [cam_extrinsic], net, tsdf_t, min_measured_depth, max_measured_depth)\n",
    "    if surface_points_combined is None:\n",
    "        surface_points_combined = surf_points_world[0, surf_mask[0]]\n",
    "        sem_surface_points_combined = sem_points_out[0, surf_mask[0]]\n",
    "    else:\n",
    "        surface_points_combined = torch.cat([surface_points_combined, surf_points_world[0, surf_mask[0]]], dim=0)\n",
    "        sem_surface_points_combined = torch.cat([sem_surface_points_combined, sem_points_out[0, surf_mask[0]]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671e60e0a4a641b094f4f0f0f684a997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PointCloud with 1557 points."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# surface_points_0 = surf_points_world[0, surf_mask[0]]\n",
    "# Debug\n",
    "surf_pc = o3d.geometry.PointCloud()\n",
    "surf_pc.points = o3d.utility.Vector3dVector(surface_points_combined)\n",
    "down_surf_pc = surf_pc\n",
    "down_surf_pc = surf_pc.voxel_down_sample(voxel_size=scene_voxel_downsample_size) # 5mm\n",
    "# # If more than max points, uniformly sample\n",
    "# if len(down_surf_pc.points) > max_points:\n",
    "#     indices = np.random.choice(np.arange(len(down_surf_pc.points)), max_points, replace=False)\n",
    "#     down_surf_pc = down_surf_pc.select_by_index(indices)\n",
    "down_surf_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.0, 0.2, 1]), (np.asarray(down_surf_pc.points).shape[0], 1)))\n",
    "visualizer6 = JVisualizer()\n",
    "visualizer6.add_geometry(down_surf_pc)\n",
    "seen_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.0, 0.0, 0.0]), (np.asarray(seen_pc.points).shape[0], 1)))\n",
    "visualizer6.add_geometry(seen_pc)\n",
    "# visualizer6.add_geometry(pc_full)\n",
    "visualizer6.show()\n",
    "down_surf_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_pc = seen_pc.voxel_down_sample(voxel_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in Knife\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a14350e73ab4afe9a48e7db60d23d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in Bag\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21931b2acc9a4eb28960462a7b911610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in Scissors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa196c0a11b4ce99691152f303e7b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in Mug\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb47562936894b37b8a2cfb934a7db28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in Earphone\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f62bfc937a425fb0ac8697c9b4fc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in Hat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2bd66e444044a0b332df132ecd1e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 2 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALl sem points Hat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006abc93d8a54413bcdee437cfc51a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 7 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vgn.utils.implicit import semantic_label_dict\n",
    "\n",
    "# argmax semantics to get semantic labels\n",
    "sem_labels_points = torch.argmax(sem_surface_points_combined, dim=1)\n",
    "\n",
    "visualizer7 = JVisualizer()\n",
    "# visualizer7.add_geometry(pc_full)\n",
    "# downsample seen pc to one cm\n",
    "visualizer7.add_geometry(seen_pc)\n",
    "\n",
    "for sem_class in semantic_label_dict.keys():\n",
    "    sem_class_mask = sem_labels_points == semantic_label_dict[sem_class]\n",
    "\n",
    "    # if some points exist:\n",
    "    if sem_class_mask.sum() > 0:\n",
    "        # visualize\n",
    "        sem_class_pc = o3d.geometry.PointCloud()\n",
    "        sem_class_pc.points = o3d.utility.Vector3dVector(surface_points_combined[sem_class_mask])\n",
    "        rand_color = np.random.uniform(0,1,3)\n",
    "        sem_class_pc.colors = o3d.utility.Vector3dVector(np.tile(rand_color, (np.asarray(sem_class_pc.points).shape[0], 1)))\n",
    "        visualizer7.add_geometry(sem_class_pc)\n",
    "\n",
    "        sep_viz = JVisualizer()\n",
    "        sep_viz.add_geometry(sem_class_pc)\n",
    "        sep_viz.add_geometry(seen_pc)\n",
    "        print(\"points in:\", sem_class)\n",
    "        sep_viz.show()\n",
    "\n",
    "print(\"ALl sem points:\")\n",
    "visualizer7.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GIGA-6DoF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "838152720d4d6971dc8654fbdb394087b2421f5e78d77e4d8c651076f26533d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
