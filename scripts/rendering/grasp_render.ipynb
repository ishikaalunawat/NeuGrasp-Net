{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import open3d as o3d\n",
    "from open3d import JVisualizer\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "# import scipy.signal as signal\n",
    "# from tqdm import tqdm\n",
    "# import multiprocessing as mp\n",
    "\n",
    "# from vgn.grasp import Grasp, Label\n",
    "from vgn.io import *\n",
    "from vgn.perception import *\n",
    "from vgn.simulation import ClutterRemovalSim\n",
    "from vgn.utils.transform import Rotation, Transform\n",
    "from vgn.utils.implicit import get_scene_from_mesh_pose_list, as_mesh\n",
    "from vgn.utils.misc import apply_noise\n",
    "from vgn.grasp_sampler import GpgGraspSamplerPcl\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "previous_root=\"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/pile/data_pile_train_random_raw_4M_radomized_views/\"\n",
    "data_root = \"/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/\"\n",
    "sim_gui = False\n",
    "three_cameras = True # Use one camera for wrist and two cameras for the fingers\n",
    "add_noise = True # Add dex noise to the rendered images like GIGA\n",
    "noise_type = 'mod_dex'\n",
    "gp_rate = 0.5 # Rate of applying Gaussian process noise\n",
    "voxel_downsample_size = 0.002 # 2mm\n",
    "max_points = 1023\n",
    "scene='pile'\n",
    "object_set='pile/train'\n",
    "\n",
    "## Re-create the saved simulation\n",
    "# Get random scene\n",
    "index = np.random.randint(32000) # index 20 is a good example.\n",
    "mesh_list_files = glob.glob(os.path.join(previous_root, 'mesh_pose_list', '*.npz'))\n",
    "mesh_pose_list = np.load(mesh_list_files[index], allow_pickle=True)['pc']\n",
    "scene_id = os.path.basename(mesh_list_files[index])[:-4] # scene id without .npz extension\n",
    "## Get specific scene\n",
    "# scene_id = 'f614e39ed9df4e1094d569cddc20979b'\n",
    "# mesh_list_file = os.path.join(previous_root, 'mesh_pose_list', scene_id + '.npz')\n",
    "# mesh_pose_list = np.load(mesh_list_file, allow_pickle=True)['pc']\n",
    "\n",
    "sim = ClutterRemovalSim(scene, object_set, gui=sim_gui, data_root=data_root) # parameters scene and object_set are not used\n",
    "sim.setup_sim_scene_from_mesh_pose_list(mesh_pose_list, table=False, data_root=data_root) # Setting table to False because we don't want to render it\n",
    "# sim.save_state()\n",
    "\n",
    "# Get scene point cloud and normals using ground truth meshes\n",
    "scene_mesh = get_scene_from_mesh_pose_list(mesh_pose_list, data_root=data_root)\n",
    "o3d_scene_mesh = scene_mesh.as_open3d\n",
    "o3d_scene_mesh.compute_vertex_normals()\n",
    "pc = o3d_scene_mesh.sample_points_uniformly(number_of_points=1000)\n",
    "points = np.asarray(pc.points)\n",
    "# pc_trimesh = trimesh.points.PointCloud(points)\n",
    "# pc_colors = np.array([trimesh.visual.random_color() for i in points])\n",
    "# pc_trimesh.vertices_color = pc_colors\n",
    "# trimesh.Scene([scene_mesh, pc_trimesh]).show()\n",
    "# o3d.visualization.draw_geometries([pc])\n",
    "visualizer = JVisualizer()\n",
    "pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0, 0]), (np.asarray(pc.points).shape[0], 1)))\n",
    "visualizer.add_geometry(pc)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_side_images(sim, n=1, random=False, camera=None):\n",
    "    if camera is None:\n",
    "        camera = sim.camera\n",
    "    height, width = camera.intrinsic.height, camera.intrinsic.width\n",
    "    origin = Transform(Rotation.identity(), np.r_[sim.size / 2, sim.size / 2, sim.size / 3])\n",
    "\n",
    "    extrinsics = np.empty((n, 7), np.float32)\n",
    "    depth_imgs = np.empty((n, height, width), np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        if random:\n",
    "            r = np.random.uniform(1.6, 2.4) * sim.size\n",
    "            theta = np.random.uniform(np.pi / 4.0, 5.0 * np.pi / 12.0)\n",
    "            phi = np.random.uniform(- 5.0 * np.pi / 5, - 3.0 * np.pi / 8.0)\n",
    "        else:\n",
    "            r = 2 * sim.size\n",
    "            theta = np.pi / 3.0\n",
    "            phi = - np.pi / 2.0\n",
    "\n",
    "        extrinsic = camera_on_sphere(origin, r, theta, phi)\n",
    "        depth_img = camera.render(extrinsic)[1]\n",
    "\n",
    "        extrinsics[i] = extrinsic.to_list()\n",
    "        depth_imgs[i] = depth_img\n",
    "\n",
    "    return depth_imgs, extrinsics\n",
    "# Create our own camera\n",
    "width, height = 64, 64 # relatively low resolution\n",
    "width_fov = np.deg2rad(120.0) # angular FOV\n",
    "height_fov = np.deg2rad(120.0) # angular FOV\n",
    "f_x = width / (np.tan(width_fov / 2.0))\n",
    "f_y = height / (np.tan(height_fov / 2.0))\n",
    "intrinsic = CameraIntrinsic(width, height, f_x, f_y, width/2, height/2)\n",
    "\n",
    "# To capture 5cms on both sides of the gripper, using a 120 deg FOV, we need to be atleast 0.05/tan(60) = 2.8 cms away\n",
    "height_max_dist = sim.gripper.max_opening_width/2.5\n",
    "width_max_dist  = sim.gripper.max_opening_width/2.0 + 0.015 # 1.5 cm extra\n",
    "dist_from_gripper = width_max_dist/np.tan(width_fov/2.0) \n",
    "min_measured_dist = 0.001\n",
    "max_measured_dist = dist_from_gripper + sim.gripper.finger_depth + 0.005 # 0.5 cm extra\n",
    "camera = sim.world.add_camera(intrinsic, min_measured_dist, max_measured_dist+0.05) # adding 5cm extra for now but will filter it below\n",
    "# depth_imgs_side, extrinsics_side = render_side_images(sim, 1, camera=camera)\n",
    "# plt.imshow(depth_imgs_side[0]) # Normally too far away to see anything\n",
    "\n",
    "if three_cameras:\n",
    "    # Use one camera for wrist and two cameras for the fingers\n",
    "    # finger_height_max_dist = sim.gripper.max_opening_width/2.5 # Not required if filtering combined cloud\n",
    "    finger_width_max_dist = sim.gripper.finger_depth/2.0 + 0.005 # 0.5 cm extra\n",
    "    dist_from_finger = finger_width_max_dist/np.tan(width_fov/2.0)\n",
    "    finger_max_measured_dist = dist_from_finger + 0.95*sim.gripper.max_opening_width\n",
    "    finger_camera  = sim.world.add_camera(intrinsic, min_measured_dist, finger_max_measured_dist+0.05) # adding 5cm extra for now but will filter it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample grasps with GPG:\n",
    "sampler = GpgGraspSamplerPcl(sim.gripper.finger_depth-0.0075) # Franka finger depth is actually a little less than 0.05\n",
    "safety_dist_above_table = sim.gripper.finger_depth # table is spawned at finger_depth\n",
    "grasps, grasps_pos, grasps_rot = sampler.sample_grasps(pc, num_grasps=1, max_num_samples=180,\n",
    "                                    safety_dis_above_table=safety_dist_above_table, show_final_grasps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz grasps\n",
    "grasps_scene = trimesh.Scene()\n",
    "from vgn.utils import visual\n",
    "grasp_mesh_list = [visual.grasp2mesh(g) for g in grasps]\n",
    "for i, g_mesh in enumerate(grasp_mesh_list):\n",
    "    grasps_scene.add_geometry(g_mesh, node_name=f'grasp_{i}')\n",
    "    break\n",
    "# grasps_scene.show()\n",
    "composed_scene = trimesh.Scene([scene_mesh, grasps_scene])\n",
    "composed_scene.show()\n",
    "# visualizer.add_geometry(as_mesh(grasps_scene).as_open3d)\n",
    "# o3d.visualization.draw_geometries([pc, as_mesh(grasps_scene).as_open3d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Move camera to grasp offset frame\n",
    "grasp_center = grasps[0].pose.translation\n",
    "# Unfortunately VGN/GIGA grasps are not in the grasp frame we want (frame similar to PointNetGPD), so we need to transform them\n",
    "grasp_frame_rot =  grasps[0].pose.rotation * Rotation.from_euler('Y', np.pi/2) * Rotation.from_euler('Z', np.pi)\n",
    "grasp_tf = Transform(grasp_frame_rot, grasp_center).as_matrix()\n",
    "offset_pos =  (grasp_tf @ np.array([[-dist_from_gripper],[0],[0],[1.]]))[:3].squeeze() # Move to offset frame\n",
    "# (Debug) viz the grasp center and offset_pos:\n",
    "# visualizer.add_geometry(o3d.geometry.PointCloud(o3d.utility.Vector3dVector(grasp_center.reshape(1,3))))\n",
    "# visualizer.add_geometry(o3d.geometry.PointCloud(o3d.utility.Vector3dVector(offset_pos.reshape(1,3))))\n",
    "# visualizer.show()\n",
    "\n",
    "# Unfortunately the bullet renderer uses the OpenGL format so we have to use yet another extrinsic\n",
    "grasp_up_axis = grasp_tf.T[2,:3] # np.array([0.0, 0.0, 1.0]) # grasp_tf z-axis\n",
    "extrinsic_bullet = Transform.look_at(eye=offset_pos, center=grasp_center, up=grasp_up_axis)\n",
    "\n",
    "## render image\n",
    "depth_img = camera.render(extrinsic_bullet)[1]\n",
    "# Optional: Add some dex noise like GIGA\n",
    "if add_noise:\n",
    "    depth_img = apply_noise(depth_img, noise_type=noise_type, gp_rate=gp_rate)\n",
    "plt.imshow(depth_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the same for the other cameras\n",
    "if three_cameras:\n",
    "    ## Move camera to finger offset frame\n",
    "    fingers_center =  (grasp_tf @ np.array([[sim.gripper.finger_depth/2.0],[0],[0],[1.]]))[:3].squeeze()\n",
    "    left_finger_offset_pos  =  (grasp_tf @ np.array([[sim.gripper.finger_depth/2.0],[ (dist_from_finger + sim.gripper.max_opening_width/2.0)],[0],[1.]]))[:3].squeeze()\n",
    "    right_finger_offset_pos =  (grasp_tf @ np.array([[sim.gripper.finger_depth/2.0],[-(dist_from_finger + sim.gripper.max_opening_width/2.0)],[0],[1.]]))[:3].squeeze()\n",
    "    \n",
    "    # Unfortunately the bullet renderer uses the OpenGL format so we have to use yet another extrinsic\n",
    "    left_finger_extrinsic_bullet  = Transform.look_at(eye=left_finger_offset_pos,  center=fingers_center, up=grasp_up_axis)\n",
    "    right_finger_extrinsic_bullet = Transform.look_at(eye=right_finger_offset_pos, center=fingers_center, up=grasp_up_axis)\n",
    "\n",
    "    ## render image\n",
    "    left_finger_depth_img  = finger_camera.render(left_finger_extrinsic_bullet )[1]\n",
    "    right_finger_depth_img = finger_camera.render(right_finger_extrinsic_bullet)[1]\n",
    "    # Optional: Add some dex noise like GIGA\n",
    "    if add_noise:\n",
    "        left_finger_depth_img = apply_noise(left_finger_depth_img, noise_type=noise_type, gp_rate=gp_rate)\n",
    "        right_finger_depth_img = apply_noise(right_finger_depth_img, noise_type=noise_type, gp_rate=gp_rate)\n",
    "    # Viz\n",
    "    plt.imshow(left_finger_depth_img)\n",
    "    plt.imshow(right_finger_depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Convert to point cloud (TODO: Do this yourself with filtering)\n",
    "# camera_mat_o3d = o3d.camera.PinholeCameraIntrinsic(128, 128, intrinsic.K[0,0], intrinsic.K[1,1], intrinsic.K[0,2], intrinsic.K[1,2])\n",
    "# grasp_cam_local_depth_pc = o3d.geometry.PointCloud.create_from_depth_image(o3d.geometry.Image(depth_img), camera_mat_o3d, extrinsic_bullet.as_matrix(), depth_scale=1.0)\n",
    "# # grasp_cam_local_depth_pc = o3d.geometry.PointCloud.create_from_depth_image(o3d.geometry.Image(depth_img), camera_mat_o3d, depth_scale=1.0)\n",
    "\n",
    "\n",
    "# ## Viz point cloud and grasp\n",
    "# grasp_cam_local_depth_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0, 1]), (np.asarray(grasp_cam_local_depth_pc.points).shape[0], 1)))\n",
    "# visualizer = JVisualizer()\n",
    "# visualizer.add_geometry(grasp_cam_local_depth_pc)\n",
    "\n",
    "# if three_cameras:\n",
    "#     left_cam_local_depth_pc  = o3d.geometry.PointCloud.create_from_depth_image(o3d.geometry.Image(left_finger_depth_img),  camera_mat_o3d, left_finger_extrinsic_bullet.as_matrix(),  depth_scale=1.0)\n",
    "#     right_cam_local_depth_pc = o3d.geometry.PointCloud.create_from_depth_image(o3d.geometry.Image(right_finger_depth_img), camera_mat_o3d, right_finger_extrinsic_bullet.as_matrix(), depth_scale=1.0)\n",
    "#     left_cam_local_depth_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([1, 0, 0]), (np.asarray(left_cam_local_depth_pc.points).shape[0], 1)))\n",
    "#     right_cam_local_depth_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 1, 0]), (np.asarray(right_cam_local_depth_pc.points).shape[0], 1)))\n",
    "#     visualizer.add_geometry(left_cam_local_depth_pc)\n",
    "#     visualizer.add_geometry(right_cam_local_depth_pc)\n",
    "\n",
    "# visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to point cloud\n",
    "pixel_grid = np.meshgrid(np.arange(width), np.arange(height))\n",
    "pixels = np.dstack((pixel_grid[0],pixel_grid[1])).reshape(-1, 2)\n",
    "\n",
    "# depth_eps = 0.0001\n",
    "depth_array = depth_img.reshape(-1)\n",
    "relevant_mask = depth_array < (max_measured_dist) #- depth_eps) # only depth values in range\n",
    "\n",
    "filt_pixels = np.array(pixels[relevant_mask]) # only consider pixels with depth values in range\n",
    "filt_pixels = np.hstack((filt_pixels, np.ones((filt_pixels.shape[0], 2)))) # Homogenous co-ordinates\n",
    "# Project pixels into camera space\n",
    "filt_pixels[:,:3] *= depth_array[relevant_mask].reshape(-1, 1) # Multiply by depth\n",
    "intrinsic_hom = np.eye(4)\n",
    "intrinsic_hom[:3,:3] = intrinsic.K\n",
    "p_local = np.linalg.inv(intrinsic_hom) @ filt_pixels.T\n",
    "# Also filter out points that are more than max dist height # Not required if filtering combined cloud\n",
    "# p_local = p_local[:, p_local[1,:] <  height_max_dist]\n",
    "# p_local = p_local[:, p_local[1,:] > -height_max_dist]\n",
    "p_world = np.linalg.inv(extrinsic_bullet.as_matrix()) @ p_local\n",
    "\n",
    "## Viz point cloud and grasp\n",
    "# grasp_cam_local_depth_pc = o3d.geometry.PointCloud()\n",
    "# grasp_cam_local_depth_pc.points = o3d.utility.Vector3dVector(p_local[:3,:].T)\n",
    "grasp_cam_world_depth_pc = o3d.geometry.PointCloud()\n",
    "grasp_cam_world_depth_pc.points = o3d.utility.Vector3dVector(p_world[:3,:].T)\n",
    "# grasp_cam_local_depth_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 1, 1]), (np.asarray(grasp_cam_local_depth_pc.points).shape[0], 1)))\n",
    "grasp_cam_world_depth_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 0, 1]), (np.asarray(grasp_cam_world_depth_pc.points).shape[0], 1)))\n",
    "# visualizer = JVisualizer()\n",
    "# visualizer.add_geometry(grasp_cam_local_depth_pc)\n",
    "visualizer.add_geometry(grasp_cam_world_depth_pc)\n",
    "# viz original pc and gripper\n",
    "o3d_gripper_mesh = as_mesh(grasps_scene).as_open3d\n",
    "gripper_pc = o3d_gripper_mesh.sample_points_uniformly(number_of_points=3000)\n",
    "gripper_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([1, 1, 0]), (np.asarray(gripper_pc.points).shape[0], 1)))\n",
    "visualizer.add_geometry(gripper_pc)\n",
    "\n",
    "if three_cameras:\n",
    "    left_finger_depth_array = left_finger_depth_img.reshape(-1)\n",
    "    left_relevant_mask = left_finger_depth_array < (finger_max_measured_dist)# - depth_eps) # only depth values in range\n",
    "    left_filt_pixels = np.array(pixels[left_relevant_mask]) # only consider pixels with depth values in range\n",
    "    \n",
    "    left_filt_pixels = np.hstack((left_filt_pixels, np.ones((left_filt_pixels.shape[0], 2)))) # Homogenous co-ordinates\n",
    "    # Project pixels into camera space\n",
    "    left_filt_pixels[:,:3] *= left_finger_depth_array[left_relevant_mask].reshape(-1, 1) # Multiply by depth\n",
    "    left_p_local = np.linalg.inv(intrinsic_hom) @ left_filt_pixels.T\n",
    "    # Also filter out points that are more than max dist height and width # Not required if filtering combined cloud\n",
    "    # left_p_local = left_p_local[:, left_p_local[0,:] <  finger_width_max_dist]\n",
    "    # left_p_local = left_p_local[:, left_p_local[0,:] > -finger_width_max_dist]\n",
    "    # left_p_local = left_p_local[:, left_p_local[1,:] <  finger_height_max_dist]\n",
    "    # left_p_local = left_p_local[:, left_p_local[1,:] > -finger_height_max_dist]\n",
    "    left_p_world = np.linalg.inv(left_finger_extrinsic_bullet.as_matrix()) @ left_p_local\n",
    "\n",
    "    right_finger_depth_array = right_finger_depth_img.reshape(-1)\n",
    "    right_relevant_mask = right_finger_depth_array < (finger_max_measured_dist)# - depth_eps) # only depth values in range\n",
    "    right_filt_pixels = np.array(pixels[right_relevant_mask]) # only consider pixels with depth values in range\n",
    "    \n",
    "    right_filt_pixels = np.hstack((right_filt_pixels, np.ones((right_filt_pixels.shape[0], 2)))) # Homogenous co-ordinates\n",
    "    # Project pixels into camera space\n",
    "    right_filt_pixels[:,:3] *= right_finger_depth_array[right_relevant_mask].reshape(-1, 1) # Multiply by depth\n",
    "    right_p_local = np.linalg.inv(intrinsic_hom) @ right_filt_pixels.T\n",
    "    # Also filter out points that are more than max dist height and width # Not required if filtering combined cloud\n",
    "    # right_p_local = right_p_local[:, right_p_local[0,:] <  finger_width_max_dist]\n",
    "    # right_p_local = right_p_local[:, right_p_local[0,:] > -finger_width_max_dist]\n",
    "    # right_p_local = right_p_local[:, right_p_local[1,:] <  finger_height_max_dist]\n",
    "    # right_p_local = right_p_local[:, right_p_local[1,:] > -finger_height_max_dist]\n",
    "    right_p_world = np.linalg.inv(right_finger_extrinsic_bullet.as_matrix()) @ right_p_local    \n",
    "\n",
    "    # Viz\n",
    "    left_cam_world_depth_pc = o3d.geometry.PointCloud()\n",
    "    left_cam_world_depth_pc.points = o3d.utility.Vector3dVector(left_p_world[:3,:].T)\n",
    "    right_cam_world_depth_pc = o3d.geometry.PointCloud()\n",
    "    right_cam_world_depth_pc.points = o3d.utility.Vector3dVector(right_p_world[:3,:].T)\n",
    "\n",
    "    left_cam_world_depth_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([1, 0, 0]), (np.asarray(left_cam_world_depth_pc.points).shape[0], 1)))\n",
    "    right_cam_world_depth_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0, 1, 0]), (np.asarray(right_cam_world_depth_pc.points).shape[0], 1)))\n",
    "    visualizer.add_geometry(left_cam_world_depth_pc)\n",
    "    visualizer.add_geometry(right_cam_world_depth_pc)\n",
    "\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and downsample surface point cloud\n",
    "combined_world_points = np.hstack((p_world, left_p_world, right_p_world))\n",
    "# filter points that are too far away\n",
    "combined_world_points_local = np.linalg.inv(grasp_tf) @ combined_world_points\n",
    "inval_mask = combined_world_points_local[0,:] > (max_measured_dist - dist_from_gripper) # too far X\n",
    "inval_mask = inval_mask | (combined_world_points_local[0,:] < -dist_from_gripper)       # too close X\n",
    "inval_mask = inval_mask | (combined_world_points_local[2,:] >  height_max_dist)         # too far Z\n",
    "inval_mask = inval_mask | (combined_world_points_local[2,:] < -height_max_dist)         # too close Z\n",
    "combined_world_points_filt = combined_world_points[:, ~inval_mask]\n",
    "\n",
    "combined_pc = o3d.geometry.PointCloud()\n",
    "combined_pc.points = o3d.utility.Vector3dVector(combined_world_points_filt[:3,:].T)\n",
    "\n",
    "down_combined_pc = combined_pc.voxel_down_sample(voxel_size=voxel_downsample_size)\n",
    "# If more than max points, uniformly sample\n",
    "if len(down_combined_pc.points) > max_points:\n",
    "    indices = np.random.choice(np.arange(len(down_combined_pc.points)), max_points, replace=False)\n",
    "    down_combined_pc = down_combined_pc.select_by_index(indices)\n",
    "down_combined_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([1.0, 0.45, 0.]), (np.asarray(down_combined_pc.points).shape[0], 1)))\n",
    "visualizer2 = JVisualizer()\n",
    "visualizer2.add_geometry(down_combined_pc)\n",
    "# viz original pc and gripper\n",
    "visualizer2.add_geometry(pc)\n",
    "visualizer2.add_geometry(gripper_pc)\n",
    "visualizer2.show()\n",
    "down_combined_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grasp_pc_path = '/home/sjauhri/IAS_WS/potato-net/GIGA-TSDF/GIGA-6DoF/data/pile/data_pile_train_random_raw_HiRes_tiny/grasp_point_clouds/10.npz'\n",
    "# grasp_pc = np.load(grasp_pc_path)['pc']\n",
    "# temp_pc = o3d.geometry.PointCloud()\n",
    "# temp_pc.points = o3d.utility.Vector3dVector(grasp_pc)\n",
    "# temp_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.0, 0.45, 0.8]), (np.asarray(temp_pc.points).shape[0], 1)))\n",
    "# visualizer2.add_geometry(temp_pc)\n",
    "# visualizer2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check occupancy of grasp surf points\n",
    "from vgn.utils.implicit import get_scene_from_mesh_pose_list\n",
    "from vgn.ConvONets.utils.libmesh import check_mesh_contains\n",
    "\n",
    "def get_occ_specific_points(mesh_list, points):\n",
    "    num_point = points.shape[0]\n",
    "    occ = np.zeros(num_point).astype(bool)\n",
    "    for mesh in mesh_list:\n",
    "        occi = check_mesh_contains(mesh, points)\n",
    "        occ = occ | occi\n",
    "    \n",
    "    return points, occ\n",
    "\n",
    "points = np.asarray(down_combined_pc.points)\n",
    "# for mesh_path, scale, pose in mesh_pose_list:\n",
    "    # mesh_path = os.path.join(data_root, mesh_path)\n",
    "for i in range(len(mesh_pose_list)):\n",
    "    mesh_pose_list[i][0] = os.path.join(data_root, mesh_pose_list[i][0])\n",
    "scene, mesh_list = get_scene_from_mesh_pose_list(mesh_pose_list, return_list=True)\n",
    "points, occ = get_occ_specific_points(mesh_list, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape, occ.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_pc = o3d.geometry.PointCloud()\n",
    "not_occ_pc = o3d.geometry.PointCloud()\n",
    "occ_pc.points = o3d.utility.Vector3dVector(points[occ])\n",
    "not_occ_pc.points = o3d.utility.Vector3dVector(points[~occ])\n",
    "\n",
    "occ_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.0, 0.45, 0.9]), (np.asarray(occ_pc.points).shape[0], 1)))\n",
    "not_occ_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.4, 0.9, 0.45]), (np.asarray(not_occ_pc.points).shape[0], 1)))\n",
    "visualizer3 = JVisualizer()\n",
    "# visualizer3.add_geometry(down_combined_pc)\n",
    "# # viz original pc and gripper\n",
    "# visualizer3.add_geometry(pc)\n",
    "visualizer3.add_geometry(gripper_pc)\n",
    "visualizer3.add_geometry(occ_pc)\n",
    "visualizer3.add_geometry(not_occ_pc)\n",
    "visualizer3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637b82d2402a42109154d2987be0ab82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 3 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "id = 2097097\n",
    "occ_data = np.load('/work/scratch/sj93qicy/potato-net/data/pile/data_pile_train_random_raw_2M_GPG_MIXED/occ_grasp_point_clouds_noisy/'+str(id)+'.npz')\n",
    "points = occ_data['points']\n",
    "occ = occ_data['occ']\n",
    "grasp_pc_data = np.load('/work/scratch/sj93qicy/potato-net/data/pile/data_pile_train_random_raw_2M_GPG_MIXED/grasp_point_clouds_gt/'+str(id)+'.npz')\n",
    "grasp_pc = grasp_pc_data['pc']\n",
    "import open3d as o3d\n",
    "from open3d import JVisualizer\n",
    "visualizer4 = JVisualizer()\n",
    "occ_pc = o3d.geometry.PointCloud()\n",
    "not_occ_pc = o3d.geometry.PointCloud()\n",
    "occ_pc.points = o3d.utility.Vector3dVector(points[occ])\n",
    "not_occ_pc.points = o3d.utility.Vector3dVector(points[~occ])\n",
    "grasp_pc_o3d = o3d.geometry.PointCloud()\n",
    "grasp_pc_o3d.points = o3d.utility.Vector3dVector(grasp_pc)\n",
    "\n",
    "occ_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.0, 0.45, 0.9]), (np.asarray(occ_pc.points).shape[0], 1)))\n",
    "not_occ_pc.colors = o3d.utility.Vector3dVector(np.tile(np.array([0.4, 0.9, 0.45]), (np.asarray(not_occ_pc.points).shape[0], 1)))\n",
    "grasp_pc_o3d.colors = o3d.utility.Vector3dVector(np.tile(np.array([1.0, 0.45, 0.0]), (np.asarray(grasp_pc_o3d.points).shape[0], 1)))\n",
    "visualizer4.add_geometry(occ_pc)\n",
    "visualizer4.add_geometry(not_occ_pc)\n",
    "visualizer4.add_geometry(grasp_pc_o3d)\n",
    "visualizer4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fdf901c31fc67ed55bbb90f38943bb70f17cb0eef6cbf70e1531a438eebd547"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
